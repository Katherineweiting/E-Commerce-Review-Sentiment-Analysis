# -*- coding: utf-8 -*-
"""Final_Project_multiclass_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12F7Qm88FM1iGFHFsel9TovwFiK4WdVWm

# Women E-Commerce Review Sentiment Analysis

---
### Load in and visualize the data
"""

import pandas as pd

# read data from text files
review_df = pd.read_csv('Reviews.csv')
review_df

review_df.info()

review_df.drop(['Unnamed: 0'], axis = 1, inplace = True)

review_df.head()

# checking for duplicated entries
duplicates = review_df.duplicated().sum()
if  duplicates == 0:
    print("There are no duplicted rows in this data")
else:
    print('There are: ', str(duplicates), " duplicates")

review_df=review_df.drop_duplicates()

# checking for Null values
review_df["Review Text"].isnull().sum()

# drop Null values
review_df.dropna(subset=['Review Text'], inplace= True)

# Add text length column as a reference for padding length
review_df['Text_Length'] = review_df['Review Text'].apply(lambda x: len(x.split()))

review_df

"""**Visualizing Data**"""

import matplotlib.pyplot as plt
import pandas as pd
import plotly.graph_objects as go
import numpy as np

recommended = review_df[review_df['Recommended IND'] == 1]
recommended_n = review_df[review_df['Recommended IND']==0]

plt.figure(figsize=(10, 6))

plt.hist(recommended['Text_Length'], bins=30, alpha=0.7, label='Recommended', color='#2166ac')
plt.hist(recommended_n['Text_Length'], bins=30, alpha=0.7, label='Unrecommended', color='#b2182b')

plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.title('Text Length by Recommended IND')
plt.legend()

plt.grid(True)
plt.show()

recommended = review_df[review_df['Recommended IND'] == 1]
not_recommended = review_df[review_df['Recommended IND'] == 0]


bins = np.linspace(min(review_df['Text_Length']), max(review_df['Text_Length']), 30)


hist_data_recommended, _ = np.histogram(recommended['Text_Length'], bins=bins)
hist_data_not_recommended, _ = np.histogram(not_recommended['Text_Length'], bins=bins)


bin_midpoints = (bins[:-1] + bins[1:]) / 2
fig = go.Figure()

fig.add_trace(go.Bar(
    x=bin_midpoints,
    y=hist_data_recommended,
    name='Recommended',
    text=hist_data_recommended,
    textposition='auto',
    marker_color='#2166ac',
    hoverinfo='y+text'
))


fig.add_trace(go.Bar(
    x=bin_midpoints,
    y=hist_data_not_recommended,
    name='Not Recommended',
    text=hist_data_not_recommended,
    textposition='auto',
    marker_color='#b2182b',
    hoverinfo='y+text'
))

fig.update_layout(
    title='Text Length by Recommended IND',
    xaxis_title='Text Length',
    yaxis_title='Frequency',
    barmode='overlay',
    legend_title='Recommendation',
)

fig.show()

# The distribution of recommend and unrecommend text length is similar. There is a surge in around 100 words.
# This will be reference for padding length choice.

# Counting the values in the 'Recommended IND' column
recommended_counts = review_df['Recommended IND'].value_counts()

# Create a pie chart
plt.figure(figsize=(8, 8))
plt.pie(recommended_counts, labels=['Recommended', 'Not Recommended'], autopct='%1.1f%%')
plt.title('Distribution of Recommendations')
plt.show()

recommended_counts = review_df['Recommended IND'].value_counts()

pie_fig = go.Figure(data=[
    go.Pie(
        labels=['Recommended', 'Not Recommended'],
        values=recommended_counts,
        marker=dict(colors=['#2166ac', '#b2182b']),
        textinfo='percent+label',
        hoverinfo='label+percent',
    )
])
pie_fig.update_layout(title='Distribution of Recommendations',)
pie_fig.show()

# Rating percentage, from 1 Worst, to 5 Best
rating_counts = review_df['Rating'].value_counts().sort_index()
total_ratings = rating_counts.sum()
rating_percentages = rating_counts / total_ratings
rating_fig = go.Figure()
rating_fig.add_trace(go.Bar(
    x=rating_counts.index,
    y=rating_counts.values,
    text=rating_percentages.apply(lambda x: '{:.1%}'.format(x)),
    textposition='outside',
    marker=dict(color='#2166ac')
))
rating_fig.update_layout(
    title='Distribution of Ratings',
    autosize=False,
    width=800,
    height=600,
    xaxis=dict(title='Rating'),
    yaxis=dict(title='Count'),
    margin=dict(l=100, r=100, b=50, t=100, pad=4)
)

classes = (
    review_df
    .groupby(['Recommended IND', 'Class Name'])
    .size()
    .to_frame()
    .rename(columns={0: 'Count'})
    .reset_index()
)
total_counts = classes.groupby('Class Name')['Count'].transform('sum')
classes['Percentage'] = classes['Count'].div(total_counts)
departments_pivot = classes.pivot(index='Class Name', columns='Recommended IND', values='Percentage')
departments_pivot = departments_pivot.fillna(0)
fig = go.Figure()

for col in departments_pivot.columns:
    fig.add_trace(go.Bar(
        y=departments_pivot.index,
        x=departments_pivot[col],
        name='Not Recommended' if col == 0 else 'Recommended',
        orientation='h',
        marker=dict(color='#b2182b' if col == 0 else '#2166ac'),
        text=departments_pivot[col].apply(lambda x: '{:.1%}'.format(x)),
        textposition='inside'
    ))


fig.update_layout(
    title='Distribution of <b>Class Name</b> by Recommendation',
    barmode='stack',
    autosize=False,
    width=680,
    height=800,
    margin=dict(l=150, r=100, b=30, t=100, pad=4),
    xaxis=dict(tickformat=',.0%'),
    showlegend=True
)

class_counts = review_df['Class Name'].value_counts()

# Create a figure with a size of 15x6 inches
plt.figure(figsize=(15, 6))

# Subplot 1: Class Distribution (Bar plot)
plt.subplot(1, 2, 1)
class_counts.plot(kind='bar')
plt.title('Class Distribution')
plt.xlabel('Class Name')
plt.ylabel('Frequency')

plt.show()

class_counts = review_df['Class Name'].value_counts()

fig = go.Figure(data=[
    go.Bar(
        x=class_counts.index,
        y=class_counts.values,
        marker_color='#2166ac'
    )
])

fig.update_layout(
    title='Class Distribution',
    xaxis=dict(
        title='Class Name',
        tickangle=-45
    ),
    yaxis=dict(
        title='Frequency'
    ),
    )

fig.show()

plt.figure(figsize=(15, 6))

# Subplot 1: Age Distribution
plt.subplot(1, 2, 1)
plt.hist(review_df['Age'], bins=30)  # Adjust the number of bins as needed
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Frequency')

plt.show()

hist_data, bin_edges = np.histogram(review_df['Age'], bins=30)

fig = go.Figure(data=[
    go.Bar(
        x=(bin_edges[:-1] + bin_edges[1:]) / 2,
        y=hist_data,
marker_color='#2166ac'
    )
])

fig.update_layout(
    title='Age Distribution',
    xaxis=dict(title='Age'),
    yaxis=dict(title='Frequency'),
    autosize=False,
    width=800,
    height=600
)
fig.show()

## Create new dataframe to calculate recommendation percentage for each age group of class category
age_ranges = [(20, 29), (30, 39), (40, 49), (50, 59), (60, 69), (70, 79)]
age_groups = ['20to29', '30to39', '40to49', '50to59', '60to69', '70to79']
for (start_age, end_age), age_group in zip(age_ranges, age_groups):
    review_df.loc[review_df['Age'].between(start_age, end_age, inclusive='right'), 'AgeGroup'] = age_group

recommendation_rates = review_df.groupby(['Class Name', 'AgeGroup'])['Recommended IND'].mean().reset_index()

grouped_data = recommendation_rates.pivot(index='Class Name', columns='AgeGroup', values='Recommended IND').fillna(0)


top_10_classes = review_df['Class Name'].value_counts().nlargest(10).index
grouped_data_top_10 = grouped_data.loc[top_10_classes]


fig = go.Figure()
for age_group in age_groups:
    fig.add_trace(go.Bar(
        x=grouped_data_top_10.index,
        y=grouped_data_top_10[age_group],
        name=age_group
    ))

fig.update_layout(
    barmode='group',
    title='Top 10 Classes by Age Group (Percentage Who Recommend)',
    xaxis_title='Class Name',
    yaxis=dict(
        title='Percentage Who Recommend',
        tickformat='.0%'
    ),
    legend_title='Age Group'
)

fig.show()

# add: 1. 不同年齡對class的推薦比例 2. 不同年齡買不同class的frequency

age_ranges = [(20, 29), (30, 39), (40, 49), (50, 59), (60, 69), (70, 79)]
age_groups = ['20to29', '30to39', '40to49', '50to59', '60to69', '70to79']

# Assign each review to an age group based on the customer's age
for (start_age, end_age), age_group in zip(age_ranges, age_groups):
    review_df.loc[review_df['Age'].between(start_age, end_age, inclusive='right'), 'AgeGroup'] = age_group

# Group by 'AgeGroup' and 'Class Name' and calculate the percentage of recommendations
age_class_rec = review_df.groupby(['AgeGroup', 'Department Name'])['Recommended IND'].agg(
    Recommendation_Rate=lambda x: x.sum() / x.count()).reset_index()

# Filter for the top 10 classes by number of reviews
top_classes = review_df['Department Name'].value_counts().nlargest(10).index
age_class_rec_top = age_class_rec[age_class_rec['Department Name'].isin(top_classes)]

# Pivot the DataFrame for the grouped bar chart
age_class_pivot = age_class_rec_top.pivot(index='AgeGroup', columns='Department Name', values='Recommendation_Rate').fillna(0)

# Create the grouped bar chart
fig = go.Figure()

# Add a bar for each class name within each age group
for class_name in top_classes:
    if class_name in age_class_pivot.columns:
        fig.add_trace(go.Bar(
            x=age_class_pivot.index,
            y=age_class_pivot[class_name] ,
            name=class_name
        ))

# Update the layout of the chart
fig.update_layout(
    barmode='group',
    title='Grouped Bar Chart of Recommendation Rate by Age Group for Department Name',
    xaxis_title='Age Group',
    yaxis=dict(
        title='Percentage Who Recommend (%)',
        tickformat='.0%'
    ),
    legend_title='Department Name'
)

# Show the plot
fig.show()

age_ranges = [(20, 29), (30, 39), (40, 49), (50, 59), (60, 69), (70, 79)]
age_groups = ['20to29', '30to39', '40to49', '50to59', '60to69', '70to79']

# Assign each review to an age group based on the customer's age
for (start_age, end_age), age_group in zip(age_ranges, age_groups):
    review_df.loc[review_df['Age'].between(start_age, end_age, inclusive='right'), 'AgeGroup'] = age_group

# Group by 'AgeGroup' and 'Class Name' and calculate the percentage of recommendations
age_class_rec = review_df.groupby(['AgeGroup', 'Class Name'])['Recommended IND'].agg(
    Recommendation_Rate=lambda x: x.sum() / x.count()).reset_index()

# Filter for the top 10 classes by number of reviews
top_classes = review_df['Class Name'].value_counts().nlargest(10).index
age_class_rec_top = age_class_rec[age_class_rec['Class Name'].isin(top_classes)]

# Pivot the DataFrame for the grouped bar chart
age_class_pivot = age_class_rec_top.pivot(index='AgeGroup', columns='Class Name', values='Recommendation_Rate').fillna(0)

# Create the grouped bar chart
fig = go.Figure()

# Add a bar for each class name within each age group
for class_name in top_classes:
    if class_name in age_class_pivot.columns:
        fig.add_trace(go.Bar(
            x=age_class_pivot.index,
            y=age_class_pivot[class_name] ,
            name=class_name
        ))

# Update the layout of the chart
fig.update_layout(
    barmode='group',
    title='Grouped Bar Chart of Recommendation Rate by Age Group for Top 10 Classes',
    xaxis_title='Age Group',
    yaxis=dict(
        title='Percentage Who Recommend (%)',
        tickformat='.0%'
    ),
    legend_title='Class Name'
)

# Show the plot
fig.show()

"""## Data pre-processing

The first step when building a neural network model is getting your data into the proper form to feed into the network. Since we're using embedding layers, we'll need to encode each word with an integer. We'll also want to clean it up a bit.

You can see an example of the reviews data above. Here are the processing steps, we'll want to take:
>* We'll want to get rid of periods and extraneous punctuation.
* Also, you might notice that the reviews are delimited with newline characters `\n`. To deal with those, I'm going to split the text into each review using `\n` as the delimiter.
* Then I can combined all the reviews back together into one big string.

First, let's remove all punctuation. Then get all the text without the newlines and split it into individual words.
"""

reviews = review_df["Review Text"]

# ----- All text -------

import string
all_reviews_text = '\n'.join(str(review).replace('\n', ' ') for review in review_df['Review Text'].dropna())

# Removing punctuation
all_reviews_text_no_punct = all_reviews_text.translate(str.maketrans('', '', string.punctuation))

all_reviews_text_no_punct

print(all_reviews_text_no_punct[:1000])

all_reviews_text = '\n'.join(str(review).replace('\n', ' ') for review in review_df['Review Text'].dropna())

from string import punctuation
import numpy as np

print(punctuation)

# get rid of punctuation
all_reviews_text = all_reviews_text_no_punct.lower()
all_text = ''.join([c if c not in punctuation else ' ' for c in all_reviews_text])

# split by new lines and spaces
reviews_split = all_text.split('\n')
all_text = ' '.join(reviews_split)
# create a list of words
words = all_text.split()

len(reviews_split)

words[:30]

# ----- Recommended Text --------
high_rating = review_df[review_df['Rating'] == 5]
high_rating_reviews_text = '\n'.join(str(review).replace('\n', ' ') for review in high_rating['Review Text'].dropna())

# Removing punctuation
high_rating_reviews_text_no_punct = high_rating_reviews_text.translate(str.maketrans('', '', string.punctuation))

# ----- Unrecommended Text --------
low_rating = review_df[review_df['Rating'] ==1]
low_rating_reviews_text = '\n'.join(str(review).replace('\n', ' ') for review in low_rating['Review Text'].dropna())

# Removing punctuation
low_rating_reviews_text_no_punct = low_rating_reviews_text.translate(str.maketrans('', '', string.punctuation))

# Recommended & Unrecommended reviews

high_rating_text = high_rating_reviews_text_no_punct.lower()
high_rating_text = ''.join([c if c not in punctuation else ' ' for c in high_rating_text])

low_rating_text = low_rating_reviews_text_no_punct.lower()
low_rating_text = ''.join([c if c not in punctuation else ' ' for c in low_rating_text])

# Sweater reviews between 20 to 60 years old with low rating
trend_lowrating = review_df[(review_df['Rating'] < 3) & (review_df['Department Name'] == "Trend") & (review_df['Age'] >= 40) & (review_df['Age'] <= 60)]
trend_lowrating_reviews_text = '\n'.join(str(review).replace('\n', ' ') for review in trend_lowrating['Review Text'].dropna())

# Removing punctuation
trend_lowrating_text_no_punct = trend_lowrating_reviews_text.translate(str.maketrans('', '', string.punctuation))

trend_lowrating_text = trend_lowrating_text_no_punct.lower()
trend_lowrating_text = ''.join([c if c not in punctuation else ' ' for c in trend_lowrating_text])

trend_lowrating

"""### Encoding the words

The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our reviews into integers so they can be passed into the network.

> **Exercise:** Now you're going to encode the words with integers. Build a dictionary that maps words to integers. Later we're going to pad our input vectors with zeros, so make sure the integers **start at 1, not 0**.
> Also, convert the reviews to integers and store the reviews in a new list called `reviews_ints`.
"""

from collections import Counter

## Build a dictionary that maps words to integers
counts = Counter(words)
vocab = sorted(counts, key=counts.get, reverse=True)
vocab_to_int = {words:i for i, words in enumerate(vocab, 1)}
#print(vocab_to_int)

## use the dict to tokenize each review in reviews_split
## store the tokenized reviews in reviews_ints (which is a list)
reviews_ints = []
for review in reviews_split:
    reviews_ints.append([vocab_to_int[word] for word in review.split()])

'''
from collections import Counter
import random

counts = Counter(words)
threshold = 1
total_count = len(words)
freqs = {word: count/total_count for word, count in counts.items()}
p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in counts}
vocab = sorted(counts, key=counts.get, reverse=True)
vocab_to_int = {words:i for i, words in enumerate(vocab, 1)}

reviews_ints = []
for review in reviews_split:
  for word in review.split():
    if random.random() < (1 - p_drop[word]):
      reviews_ints.append([vocab_to_int[word]])

'''

# Tried subsampling but didn't get good result, maybe because there are less words in the vocabulary

"""**Test your code**

As a text that you've implemented the dictionary correctly, print out the number of unique words in your vocabulary and the contents of the first, tokenized review.
"""

# stats about vocabulary
print('Unique words: ', len((vocab_to_int)))
print()

# print tokens in first review
print('Tokenized review: \n', reviews_ints[:1])

"""### Encoding the labels

Our labels are "positive" or "negative". To use these labels in our network, we need to convert them to 0 and 1.

Convert labels from `positive` and `negative` to 1 and 0, respectively, and place those in a new list, `encoded_labels`.
"""

# Rating labels
encoded_labels = np.array(review_df['Rating'])-1

encoded_labels.min()

encoded_labels.dtype

len(encoded_labels)

"""
### Padding sequences

"""

def pad_features(reviews_ints, seq_length):
    ''' Return features of review_ints, where each review is padded with 0's
        or truncated to the input seq_length.
    '''

    # getting the correct rows x cols shape
    features = np.zeros((len(reviews_ints), seq_length), dtype=int)

    # for each review, I grab that review and
    for i, row in enumerate(reviews_ints):
        features[i, -len(row):] = np.array(row)[:seq_length]

    return features

# Test your implementation!

seq_length = 100

features = pad_features(reviews_ints, seq_length=seq_length)

## test statements - do not change - ##
assert len(features)==len(reviews_ints), "Your features should have as many rows as reviews."
assert len(features[0])==seq_length, "Each feature row should contain seq_length values."

# print first 10 values of the first 30 batches
print(features[:30,:10])

"""## Training, Validation, Test

With our data in nice shape, we'll split it into training, validation, and test sets.

> Create the training, validation, and test sets.
* You'll need to create sets for the features and the labels, `train_x` and `train_y`, for example.
* Define a split fraction, `split_frac` as the fraction of data to **keep** in the training set. Usually this is set to 0.8 or 0.9.
* Whatever data is left will be split in half to create the validation and *testing* data.
"""

split_frac = 0.8

## split data into training, validation, and test data (features and labels, x and y)

split_idx = int(len(features)*split_frac)
train_x, remaining_x = features[:split_idx], features[split_idx:]
train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]

test_idx = int(len(remaining_x)*0.5)
val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]
val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]

## print out the shapes of your resultant feature data
print("\t\t\tFeature Shapes:")
print("Train set: \t\t{}".format(train_x.shape),
      "\nValidation set: \t{}".format(val_x.shape),
      "\nTest set: \t\t{}".format(test_x.shape))

"""# Baseline Model - Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
lr = LogisticRegression()

# Fit the model on the training data
lr.fit(train_x, train_y)

# Predict on the validation set
pred_valid = lr.predict(val_x)

# Calculate accuracy on the validation set
accuracy_valid = accuracy_score(val_y, pred_valid)
print(f"Accuracy on validation set: {accuracy_valid * 100:.2f}%")

# Predict on the test set
pred_test = lr.predict(test_x)

# Calculate accuracy on the test set
accuracy_test = accuracy_score(test_y, pred_test)
print(f"Accuracy on test set: {accuracy_test * 100:.2f}%")

"""---
## DataLoaders and Batching

"""

import torch
from torch.utils.data import TensorDataset, DataLoader

# create Tensor datasets
train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))
valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))
test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))

# dataloaders
batch_size = 100

# make sure SHUFFLE your training data
# drop_last=True will drop the last batch if the size is less than the given batch_size
train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,  drop_last=True)
valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True)
test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)

# obtain one batch of training data
dataiter = iter(train_loader)
sample_x, sample_y = next(dataiter)

print('Sample input size: ', sample_x.size()) # batch_size, seq_length
print('Sample input: \n', sample_x)
print()
print('Sample label size: ', sample_y.size()) # batch_size
print('Sample label: \n', sample_y)

# First checking if GPU is available
train_on_gpu=torch.cuda.is_available()

if(train_on_gpu):
    print('Training on GPU.')
else:
    print('No GPU available, training on CPU.')

"""# Sentiment Network (by Rating)"""

import torch.nn as nn

class SentimentRNN(nn.Module):
    """
    The RNN model that will be used to perform Sentiment analysis.
    """

    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):
        """
        Initialize the model by setting up the layers.
        """
        super(SentimentRNN, self).__init__()

        self.output_size = output_size
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim

        # Add embedding and LSTM layers
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)

        # Dropout layer
        self.dropout = nn.Dropout(0.3)

        # Linear layer
        self.fc = nn.Linear(hidden_dim, output_size)


    def forward(self, x, hidden):
        """
        Perform a forward pass of our model on some input and hidden state.
        """
        batch_size = x.size(0)

        x = x.long()

        # Compute embeddings and lstm_out
        embeds = self.embedding(x)
        lstm_out, hidden = self.lstm(embeds, hidden)

        lstm_out = lstm_out[:, -1, :]  # Getting the last time step output

        # Dropout and fully-connected layer
        out = self.dropout(lstm_out)
        out = self.fc(out)

        # Return output and hidden state
        return out, hidden


    def init_hidden(self, batch_size):
        ''' Initializes hidden state '''
        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,
        # initialized to zero, for hidden state and cell state of LSTM
        weight = next(self.parameters()).data

        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),
                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())

        return hidden

"""## Instantiate the network

Here, we'll instantiate the network. First up, defining the hyperparameters.

* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.
* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).
* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.
* `hidden_dim`: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.
* `n_layers`: Number of LSTM layers in the network. Typically between 1-3

> Define the model  hyperparameters.

"""

# Instantiate the model w/ hyperparams
vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens
output_size = 5
embedding_dim = 100
hidden_dim = 256
n_layers = 2

net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)

print(net)

"""---
## Training

Below is the typical training code. If you want to do this yourself, feel free to delete all this code and implement it yourself. You can also add code to save a model by name.

>We'll also be using a new kind of cross entropy loss, which is designed to work with a single Sigmoid output. [BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss), or **Binary Cross Entropy Loss**, applies cross entropy loss to a single value between 0 and 1.

We also have some data and training hyparameters:

* `lr`: Learning rate for our optimizer.
* `epochs`: Number of times to iterate through the training dataset.
* `clip`: The maximum gradient value to clip at (to prevent exploding gradients).
"""

# loss and optimization functions
lr = 0.005
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=1e-4)

"""**Exercise**: Fill in the missing code below to train and valid the model"""

train_losses = []
val_losses = []

epochs = 7
counter = 0
print_every = 50
clip = 5  # gradient clipping

# Move model to GPU, if available
if train_on_gpu:
    net.cuda()

net.train()

# Train for some number of epochs
for e in range(epochs):
    # Initialize hidden state
    h = net.init_hidden(batch_size)

    # Batch loop
    for inputs, labels in train_loader:
        counter += 1

        if train_on_gpu:
            inputs, labels = inputs.cuda(), labels.cuda()

        # Creating new variables for the hidden state, otherwise
        # we'd backprop through the entire training history
        h = tuple([each.data for each in h])

        # Get the output from the model
        output, h = net(inputs, h)

        # Calculate the loss and perform backpropagation
        loss = criterion(output, labels.flatten())

        # Zero accumulated gradients
        net.zero_grad()

        # Backpropagation
        loss.backward()
        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
        nn.utils.clip_grad_norm_(net.parameters(), clip)

        # Update weights
        optimizer.step()

        # Loss stats
        if counter % print_every == 0:
            # Get validation loss
            val_h = net.init_hidden(batch_size)
            tot_test_loss = 0
            net.eval()
            for inputs, labels in valid_loader:
                val_h = tuple([each.data for each in val_h])

                if train_on_gpu:
                    inputs, labels = inputs.cuda(), labels.cuda()

                output, val_h = net(inputs, val_h)
                val_loss = criterion(output, labels)
                tot_test_loss += val_loss.item()

            test_loss = tot_test_loss / len(valid_loader)

            net.train()

            # Print training and validation losses
            print("Epoch: {}/{}...".format(e + 1, epochs),
                  "Step: {}...".format(counter),
                  "Loss: {:.6f}...".format(loss.item()),
                  "Val Loss: {:.6f}".format(test_loss))

            # Append losses to the lists
            train_losses.append(loss.item())
            val_losses.append(test_loss)

# Plotting the training and validation losses
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

#Notes: change the padding length and add weight decay, also tune the learning rate and weight decay. Found out that with padding length = 150, epochs = 4, lr = 0.0005 and weight decay = 1e-4 has the best result of 62.5%
# padding length affects training when there are two many zeros or capture too less information.

"""**Note**: In the above training loop, we initialize hidden state after looping through all the batches of the training data. In this way, the hidden state computed for the previous batch is retained for the next batch. Therefore, to ensure the backpropagation work properly, we need to creating new variables for the hidden state:
        ```
        h = tuple([each.data for each in h])
        ```.

---
## Testing

There are a few ways to test your network.

* **Test data performance:** First, we'll see how our trained model performs on all of our defined test_data, above. We'll calculate the average loss and accuracy over the test data.

* **Inference on user-generated data:** Second, we'll see if we can input just one example review at a time (without a label), and see what the trained model predicts. Looking at new, user input data like this, and predicting an output label, is called **inference**.
"""

# Get test data loss and accuracy

test_losses = [] # track loss
num_correct = 0

# init hidden state
h = net.init_hidden(batch_size)

net.eval()
# iterate over test data
for inputs, labels in test_loader:

    # Creating new variables for the hidden state, otherwise
    # we'd backprop through the entire training history
    h = tuple([each.data for each in h])

    if(train_on_gpu):
        inputs, labels = inputs.cuda(), labels.cuda().unsqueeze(1)

    # get predicted outputs
    output, h = net(inputs, h)

    # calculate loss
    test_loss = criterion(output, labels.flatten())
    test_losses.append(test_loss.item())

    # convert output probabilities to predicted class (0 or 1)
    # pred = torch.round(output.squeeze())  # rounds to the nearest integer
    pred = torch.argmax(output,dim = 1)

    # compare predictions to true label
    correct_tensor = pred.eq(labels.float().view_as(pred))
    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())
    num_correct += np.sum(correct)


# -- stats! -- ##
# avg test loss
print("Test loss: {:.3f}".format(np.mean(test_losses)))

# accuracy over all test data
test_acc = num_correct/len(test_loader.dataset)
print("Test accuracy: {:.3f}".format(test_acc))

"""# Keyword Impact Analysis - Word Cloud"""

# Displaying the word frequency
word_frequency = Counter(words)

# Sorting words by frequency in descending order
sorted_word_frequency = dict(sorted(word_frequency.items(), key=lambda item: item[1], reverse=True))

# Displaying the sorted word frequency
for word, count in sorted_word_frequency.items():
  if count > len(reviews_split)*0.1 and count < len(reviews_split)*0.8:
    print(f'Word: {word} - Frequency: {count}')

from wordcloud import WordCloud
import matplotlib.pyplot as plt
wordcloud_all = WordCloud(width = 1000,
                      height = 500,
                      random_state = 21,
                      max_font_size= 119).generate(all_text)
plt.figure(figsize = (20,20), dpi= 80)
plt.imshow(wordcloud_all, interpolation= 'bilinear')
plt.axis('off')
plt.show()

wordcloud_positive = WordCloud(width = 1000,
                      height = 500,
                      random_state = 21,
                      max_font_size= 119).generate(high_rating_text)
plt.figure(figsize = (20,20), dpi= 80)
plt.imshow(wordcloud_positive, interpolation= 'bilinear')
plt.axis('off')
plt.show()

wordcloud_negative = WordCloud(width = 1000,
                      height = 500,
                      random_state = 21,
                      max_font_size= 119).generate(low_rating_text)
plt.figure(figsize = (20,20), dpi= 80)
plt.imshow(wordcloud_negative, interpolation= 'bilinear')
plt.axis('off')
plt.show()

trend_lowrating_text

wordcloud_trend = WordCloud(width = 1000,
                      height = 500,
                      random_state = 21,
                      max_font_size= 119).generate(trend_lowrating_text)
plt.figure(figsize = (20,20), dpi= 80)
plt.imshow(wordcloud_trend, interpolation= 'bilinear', cmap='winter')
plt.axis('off')
plt.show()